ğŸ–¼ï¸ Visual Question Answering (VQA) Model

Python Â· PyTorch Â· ResNet50 Â· BERT

A deep learningâ€“based Visual Question Answering (VQA) system that combines visual and textual understanding to answer natural-language questions about images.
The model is built using PyTorch, leveraging ResNet50 for image feature extraction, BERT for question encoding, and a gated fusion mechanism with spatial attention for multimodal reasoning.

ğŸ“Š Model Performance
Dataset	Hard Accuracy
VQA v2 Validation	~44.9%

Hard Accuracy: Exact match with the most frequent ground-truth answer
(Top-3000 answer classification setting)

ğŸ—ï¸ Architecture Overview
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input Image   â”‚     â”‚  Input Question â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ResNet50      â”‚     â”‚  BERT Encoder   â”‚
â”‚ (Image Encoder) â”‚     â”‚ (Text Encoder)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
              â”‚  Spatial    â”‚
              â”‚  Attention  â”‚
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
              â”‚   Gated     â”‚
              â”‚   Fusion    â”‚
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
              â”‚ Classifier  â”‚
              â”‚  (FC Layer) â”‚
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Answer    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Key Components
Component	Description
Image Encoder	ResNet50, pretrained on ImageNet
Text Encoder	BERT-base-uncased
Attention	Spatial attention over image features
Fusion	Gated fusion combining image & text features
Classifier	Fully connected layer over top-3000 answers

VQA-ResNet50-BERT/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ vqa_dataset.py        # Dataset loader
â”‚   â””â”€â”€ answer_vocab.json     # Answer â†’ index mapping
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ vqa_model.py          # Main VQA model
â”‚   â”œâ”€â”€ encoders.py           # Image & text encoders
â”‚   â”œâ”€â”€ attention.py          # Spatial attention module
â”‚   â””â”€â”€ fusion.py             # Gated fusion module
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train.py              # Training script
â”‚   â””â”€â”€ evaluate.py           # Validation evaluation
â”œâ”€â”€ inference/
â”‚   â””â”€â”€ predict.py            # Inference on custom images
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ vqa_checkpoint.pth    # Training checkpoints
â”‚   â””â”€â”€ vqa_final_model.pth   # Final trained model
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ training.ipynb
â”‚   â””â”€â”€ inference.ipynb
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

VQA-ResNet50-BERT/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ vqa_dataset.py        # Dataset loader
â”‚   â””â”€â”€ answer_vocab.json     # Answer â†’ index mapping
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ vqa_model.py          # Main VQA model
â”‚   â”œâ”€â”€ encoders.py           # Image & text encoders
â”‚   â”œâ”€â”€ attention.py          # Spatial attention module
â”‚   â””â”€â”€ fusion.py             # Gated fusion module
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train.py              # Training script
â”‚   â””â”€â”€ evaluate.py           # Validation evaluation
â”œâ”€â”€ inference/
â”‚   â””â”€â”€ predict.py            # Inference on custom images
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ vqa_checkpoint.pth    # Training checkpoints
â”‚   â””â”€â”€ vqa_final_model.pth   # Final trained model
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ training.ipynb
â”‚   â””â”€â”€ inference.ipynb
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸš€ Getting Started
Prerequisites
->Python 3.9+
->PyTorch
->CUDA-enabled GPU (recommended)
->16GB+ RAM

Installation

  git clone https://github.com/your-username/VQA-ResNet50-BERT.git
  cd VQA-ResNet50-BERT
  pip install -r requirements.txt
  
  ğŸ“¦ Dataset
  This project uses VQA v2.0 (COCO 2014).
  
  Required data:
  ->COCO 2014 train & validation images
  ->VQA train & validation questions
  ->VQA train & validation annotations
  Expected structure:
    dataset/
    â”œâ”€â”€ images/
    â”‚   â”œâ”€â”€ train2014/
    â”‚   â””â”€â”€ val2014/
    â”œâ”€â”€ questions/
    â”‚   â”œâ”€â”€ train_questions.json
    â”‚   â””â”€â”€ val_questions.json
    â””â”€â”€ annotations/
        â”œâ”€â”€ train_annotations.json
        â””â”€â”€ val_annotations.json

Training
  ->Build Answer Vocabulary
  ->Train Model
  ->Training Strategy
    ğŸ’ Freeze ResNet50 + BERT for initial epochs
    ğŸ’ Unfreeze for fine-tuning
    ğŸ’ Checkpoint saving after each epoch
    ğŸ’ Resume-safe training
      
