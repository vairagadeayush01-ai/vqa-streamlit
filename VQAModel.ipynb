{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2598787,"sourceType":"datasetVersion","datasetId":1573501},{"sourceId":14288512,"sourceType":"datasetVersion","datasetId":9120302}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-25T07:17:58.247574Z","iopub.execute_input":"2025-12-25T07:17:58.247863Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/vqa-2ansques/v2_mscoco_val2014_annotations.json\n/kaggle/input/vqa-2ansques/val_ques.json\n/kaggle/input/vqa-2ansques/v2_mscoco_train2014_annotations.json\n/kaggle/input/vqa-2ansques/Train_ques.json\n/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/5k.part\n/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/trainvalno5k.part\n/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/coco.names\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\ntrain_ans_path=\"/kaggle/input/vqa-2ansques/v2_mscoco_train2014_annotations.json\"\ntrain_ques_path=\"/kaggle/input/vqa-2ansques/Train_ques.json\"\nval_ans_path=\"/kaggle/input/vqa-2ansques/v2_mscoco_val2014_annotations.json\"\nval_ques_path=\"/kaggle/input/vqa-2ansques/val_ques.json\"\ncoco_path=\"/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:19:41.857480Z","iopub.execute_input":"2025-12-25T10:19:41.858098Z","iopub.status.idle":"2025-12-25T10:19:41.862063Z","shell.execute_reply.started":"2025-12-25T10:19:41.858069Z","shell.execute_reply":"2025-12-25T10:19:41.861387Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"os.listdir(coco_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:19:42.758704Z","iopub.execute_input":"2025-12-25T10:19:42.759570Z","iopub.status.idle":"2025-12-25T10:19:42.768831Z","shell.execute_reply.started":"2025-12-25T10:19:42.759526Z","shell.execute_reply":"2025-12-25T10:19:42.768220Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"['5k.part',\n 'labels',\n 'trainvalno5k.part',\n 'annotations',\n 'images',\n 'coco.names']"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"images_path=os.path.join(coco_path,'images')\nos.listdir(images_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:19:42.902316Z","iopub.execute_input":"2025-12-25T10:19:42.902954Z","iopub.status.idle":"2025-12-25T10:19:42.908078Z","shell.execute_reply.started":"2025-12-25T10:19:42.902925Z","shell.execute_reply":"2025-12-25T10:19:42.907484Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"['val2014', 'test2014', 'train2014']"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"train_images_dir=os.path.join(images_path,'train2014')\nval_images_dir=os.path.join(images_path,'val2014')\nprint(\"train_images_dir:\", train_images_dir,\"\\n\",\"val_images_dir:\",val_images_dir )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:19:43.136964Z","iopub.execute_input":"2025-12-25T10:19:43.137450Z","iopub.status.idle":"2025-12-25T10:19:43.141779Z","shell.execute_reply.started":"2025-12-25T10:19:43.137422Z","shell.execute_reply":"2025-12-25T10:19:43.141028Z"}},"outputs":[{"name":"stdout","text":"train_images_dir: /kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images/train2014 \n val_images_dir: /kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images/val2014\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"Checking data inside data set","metadata":{}},{"cell_type":"code","source":"paths = {\n    \"train annotations\": train_ans_path,\n    \"train questions\": train_ques_path,\n    \"val annotations\": val_ans_path,\n    \"val questions\": val_ques_path,\n    \"train images directory\": train_images_dir,\n    \"val images directory\": val_images_dir\n}\nfor name, path in paths.items():\n    if os.path.exists(path):\n        print(name, \"exists\")\n    else:\n        print(name, \"not found\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:19:44.132667Z","iopub.execute_input":"2025-12-25T10:19:44.133410Z","iopub.status.idle":"2025-12-25T10:19:44.146615Z","shell.execute_reply.started":"2025-12-25T10:19:44.133381Z","shell.execute_reply":"2025-12-25T10:19:44.145823Z"}},"outputs":[{"name":"stdout","text":"train annotations exists\ntrain questions exists\nval annotations exists\nval questions exists\ntrain images directory exists\nval images directory exists\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"\n# Load json files\nimport json\n\nwith open(train_ques_path, \"r\") as f:\n    train_questions_json = json.load(f)\n\nwith open(train_ans_path, \"r\") as f:\n    train_annotations_json = json.load(f)\n    \nprint(\"Number of train questions:\",\n      len(train_questions_json[\"questions\"]))\n\nprint(\"Number of train annotations:\",\n      len(train_annotations_json[\"annotations\"]))\n\n# Count images\n\ntrain_image_count = len(os.listdir(train_images_dir))\nval_image_count = len(os.listdir(val_images_dir))\n\nprint(\"Train images:\", train_image_count)\nprint(\"Val images:\", val_image_count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:19:44.147908Z","iopub.execute_input":"2025-12-25T10:19:44.148268Z","iopub.status.idle":"2025-12-25T10:19:49.926414Z","shell.execute_reply.started":"2025-12-25T10:19:44.148241Z","shell.execute_reply":"2025-12-25T10:19:49.925745Z"}},"outputs":[{"name":"stdout","text":"Number of train questions: 443757\nNumber of train annotations: 443757\nTrain images: 82783\nVal images: 40458\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"***Load JSON and build question_id maps***","metadata":{}},{"cell_type":"code","source":"# Load json files\nimport json\n\nwith open(train_ques_path, \"r\") as f:\n    train_questions_json = json.load(f)\n\nwith open(train_ans_path, \"r\") as f:\n    train_annotations_json = json.load(f)\n\n# question_id to string\ntrain_qid_to_question = {}\n\nfor q in train_questions_json[\"questions\"]:\n    train_qid_to_question[q[\"question_id\"]] = q[\"question\"]\n\n# Build question id to annotation map\n# key: question_id\n# value: annotation dict\n\ntrain_q2ann = {}\n\nfor ann in train_annotations_json[\"annotations\"]:\n    train_q2ann[ann[\"question_id\"]] = ann\n\n# Sanity check\nsample_qid = next(iter(train_qid_to_question))\n\nprint(\"Sample question id:\", sample_qid)\nprint(\"Question:\", train_qid_to_question[sample_qid])\nprint(\"Annotation keys:\", train_q2ann[sample_qid].keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:19:49.927640Z","iopub.execute_input":"2025-12-25T10:19:49.927943Z","iopub.status.idle":"2025-12-25T10:19:57.195105Z","shell.execute_reply.started":"2025-12-25T10:19:49.927921Z","shell.execute_reply":"2025-12-25T10:19:57.194468Z"}},"outputs":[{"name":"stdout","text":"Sample question id: 458752000\nQuestion: What is this photo taken looking through?\nAnnotation keys: dict_keys(['question_type', 'multiple_choice_answer', 'answers', 'image_id', 'answer_type', 'question_id'])\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"import os\n\n#image_id to image filename\ndef get_train_image_path(image_id):\n    image_name = f\"COCO_train2014_{image_id:012d}.jpg\"\n    return os.path.join(train_images_dir, image_name)\n\ntrain_records = []\nmissing_image_count = 0\n\nfor qid, question in train_qid_to_question.items():\n    ann = train_q2ann[qid]\n    \n    image_id = ann[\"image_id\"]\n    image_path = get_train_image_path(image_id)\n    \n    # Check if image exists\n    if not os.path.exists(image_path):\n        missing_image_count += 1\n        continue\n    \n    answers = [ans[\"answer\"] for ans in ann[\"answers\"]]\n    \n    record = {\n        \"image_path\": image_path,\n        \"question\": question,\n        \"answers\": answers\n    }\n    \n    train_records.append(record)\n\nprint(\"Total questions processed:\", len(train_qid_to_question))\nprint(\"Valid training records:\", len(train_records))\nprint(\"Missing images:\", missing_image_count)\n\nprint(\"\\nSample record\")\nprint(train_records[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:22:46.492933Z","iopub.execute_input":"2025-12-25T10:22:46.493275Z","iopub.status.idle":"2025-12-25T10:24:54.470894Z","shell.execute_reply.started":"2025-12-25T10:22:46.493246Z","shell.execute_reply":"2025-12-25T10:24:54.470001Z"}},"outputs":[{"name":"stdout","text":"Total questions processed: 443757\nValid training records: 443757\nMissing images: 0\n\nSample record\n{'image_path': '/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images/train2014/COCO_train2014_000000458752.jpg', 'question': 'What is this photo taken looking through?', 'answers': ['net', 'net', 'net', 'netting', 'net', 'net', 'mesh', 'net', 'net', 'net']}\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"# Load validation json files\nwith open(val_ques_path, \"r\") as f:\n    val_questions_json = json.load(f)\n\nwith open(val_ans_path, \"r\") as f:\n    val_annotations_json = json.load(f)\n\n# Build question id maps for validation\nval_qid_to_question = {}\nfor q in val_questions_json[\"questions\"]:\n    val_qid_to_question[q[\"question_id\"]] = q[\"question\"]\n\nval_q2ann = {}\nfor ann in val_annotations_json[\"annotations\"]:\n    val_q2ann[ann[\"question_id\"]] = ann\n\n# Function to convert image_id to validation image path\ndef get_val_image_path(image_id):\n    image_name = f\"COCO_val2014_{image_id:012d}.jpg\"\n    return os.path.join(val_images_dir, image_name)\n\n# Build validation records\nval_records = []\nmissing_val_images = 0\n\nfor qid, question in val_qid_to_question.items():\n    ann = val_q2ann[qid]\n    \n    image_id = ann[\"image_id\"]\n    image_path = get_val_image_path(image_id)\n    \n    if not os.path.exists(image_path):\n        missing_val_images += 1\n        continue\n    \n    answers = [ans[\"answer\"] for ans in ann[\"answers\"]]\n    \n    record = {\n        \"image_path\": image_path,\n        \"question\": question,\n        \"answers\": answers\n    }\n    \n    val_records.append(record)\n\nprint(\"Total validation questions:\", len(val_qid_to_question))\nprint(\"Valid validation records:\", len(val_records))\nprint(\"Missing validation images:\", missing_val_images)\n\nprint(\"\\nSample validation record\")\nprint(val_records[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:24:54.472827Z","iopub.execute_input":"2025-12-25T10:24:54.473124Z","iopub.status.idle":"2025-12-25T10:26:58.169238Z","shell.execute_reply.started":"2025-12-25T10:24:54.473099Z","shell.execute_reply":"2025-12-25T10:26:58.168621Z"}},"outputs":[{"name":"stdout","text":"Total validation questions: 214354\nValid validation records: 214124\nMissing validation images: 230\n\nSample validation record\n{'image_path': '/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images/val2014/COCO_val2014_000000262148.jpg', 'question': 'Where is he looking?', 'answers': ['down', 'down', 'at table', 'skateboard', 'down', 'table', 'down', 'down', 'down', 'down']}\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"***Answer Vocabulary & Preprocessing***","metadata":{}},{"cell_type":"code","source":"import re\n\n# Answer preprocessing\ndef preprocess_answer(ans):\n    ans = ans.lower()\n    \n    # Remove punctuation\n    ans = re.sub(r\"[^\\w\\s]\", \"\", ans)\n    \n    # Normalize numbers\n    number_map = {\n        \"zero\": \"0\",\n        \"one\": \"1\",\n        \"two\": \"2\",\n        \"three\": \"3\",\n        \"four\": \"4\",\n        \"five\": \"5\",\n        \"six\": \"6\",\n        \"seven\": \"7\",\n        \"eight\": \"8\",\n        \"nine\": \"9\",\n        \"ten\": \"10\"\n    }\n    \n    if ans in number_map:\n        ans = number_map[ans]\n    \n    # Remove extra spaces\n    ans = ans.strip()\n    \n    return ans\n\n# Quick sanity check\nprint(preprocess_answer(\"Two\"))\nprint(preprocess_answer(\"tennis-ball\"))\nprint(preprocess_answer(\"  Grey \"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:27:44.698170Z","iopub.execute_input":"2025-12-25T10:27:44.698775Z","iopub.status.idle":"2025-12-25T10:27:44.704659Z","shell.execute_reply.started":"2025-12-25T10:27:44.698744Z","shell.execute_reply":"2025-12-25T10:27:44.704029Z"}},"outputs":[{"name":"stdout","text":"2\ntennisball\ngrey\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"from collections import Counter\n\n# Count all processed answers\nanswer_counter = Counter()\n\nfor record in train_records:\n    for ans in record[\"answers\"]:\n        processed_ans = preprocess_answer(ans)\n        answer_counter[processed_ans] += 1\n\nprint(\"Total unique answers before filtering:\", len(answer_counter))\n\n# Show most common answers\nfor ans, freq in answer_counter.most_common(10):\n    print(ans,\"->\", freq)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:27:48.692276Z","iopub.execute_input":"2025-12-25T10:27:48.692591Z","iopub.status.idle":"2025-12-25T10:27:54.922969Z","shell.execute_reply.started":"2025-12-25T10:27:48.692552Z","shell.execute_reply":"2025-12-25T10:27:54.922320Z"}},"outputs":[{"name":"stdout","text":"Total unique answers before filtering: 157141\nno -> 834583\nyes -> 825618\n2 -> 122862\n1 -> 121129\nwhite -> 80731\n3 -> 62344\nred -> 48293\nblack -> 47321\nblue -> 46528\n0 -> 46338\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"# Select top 3000 answers\nTOP_K = 3000\n\nmost_common_answers = answer_counter.most_common(TOP_K)\n\nans2idx = {}\nidx2ans = []\n\nfor idx, (ans, _) in enumerate(most_common_answers):\n    ans2idx[ans] = idx\n    idx2ans.append(ans)\n\nprint(\"Final answer vocabulary size:\", len(ans2idx))\n\n# Quick check\nprint(\"Index of answer 'yes' if present:\", ans2idx.get(\"yes\", \"not found\"))\nprint(\"Answer at index 0:\", idx2ans[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:28:21.546524Z","iopub.execute_input":"2025-12-25T10:28:21.547286Z","iopub.status.idle":"2025-12-25T10:28:21.571949Z","shell.execute_reply.started":"2025-12-25T10:28:21.547253Z","shell.execute_reply":"2025-12-25T10:28:21.571339Z"}},"outputs":[{"name":"stdout","text":"Final answer vocabulary size: 3000\nIndex of answer 'yes' if present: 1\nAnswer at index 0: no\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"import torch\n\ndef build_soft_target(answers, ans2idx):\n    # Output tensor shape: (3000,)\n    target = torch.zeros(len(ans2idx))\n    \n    for ans in answers:\n        processed_ans = preprocess_answer(ans)\n        if processed_ans in ans2idx:\n            target[ans2idx[processed_ans]] += 1\n    \n    # Normalize as VQA soft score\n    target = torch.clamp(target / 10.0, max=1.0)\n    \n    return target\n\n# Test on one sample\nsample_target = build_soft_target(train_records[0][\"answers\"], ans2idx)\n\nprint(\"Target tensor shape:\", sample_target.shape)\nprint(\"Non zero entries:\", sample_target.nonzero().shape[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:28:23.769393Z","iopub.execute_input":"2025-12-25T10:28:23.769806Z","iopub.status.idle":"2025-12-25T10:28:23.776456Z","shell.execute_reply.started":"2025-12-25T10:28:23.769776Z","shell.execute_reply":"2025-12-25T10:28:23.775770Z"}},"outputs":[{"name":"stdout","text":"Target tensor shape: torch.Size([3000])\nNon zero entries: 2\n","output_type":"stream"}],"execution_count":60},{"cell_type":"markdown","source":"***Dataset and DataLoader***","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\n\nimage_transform = transforms.Compose([\n    transforms.Resize((224, 224)), # (3, 224, 224)\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:28:31.399510Z","iopub.execute_input":"2025-12-25T10:28:31.400143Z","iopub.status.idle":"2025-12-25T10:28:31.404616Z","shell.execute_reply.started":"2025-12-25T10:28:31.400112Z","shell.execute_reply":"2025-12-25T10:28:31.404049Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"\n# BERT tokenizer for TEXT\nfrom transformers import BertTokenizer\n\n# Input ids shape: (max_len,)\n# Attention mask shape: (max_len,)\n\nMAX_QUESTION_LEN = 21\n\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:28:58.003757Z","iopub.execute_input":"2025-12-25T10:28:58.004379Z","iopub.status.idle":"2025-12-25T10:28:58.399965Z","shell.execute_reply.started":"2025-12-25T10:28:58.004354Z","shell.execute_reply":"2025-12-25T10:28:58.399349Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"class VQADataset(Dataset):\n    def __init__(self, records, ans2idx, tokenizer, transform, max_len):\n        self.records = records\n        self.ans2idx = ans2idx\n        self.tokenizer = tokenizer\n        self.transform = transform\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.records)\n\n    def __getitem__(self, idx):\n        record = self.records[idx]\n\n        # Load image\n        image = Image.open(record[\"image_path\"]).convert(\"RGB\")\n        image = self.transform(image)\n        # image shape: (3, 224, 224)\n\n        # Tokenize question\n        encoding = self.tokenizer(\n            record[\"question\"],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors=\"pt\"\n        )\n\n        input_ids = encoding[\"input_ids\"].squeeze(0)\n        # input_ids shape: (max_len,)\n\n        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n        # attention_mask shape: (max_len,)\n\n        # Build soft target\n        target = build_soft_target(record[\"answers\"], self.ans2idx)\n        # target shape: (3000,)\n\n        return image, input_ids, attention_mask, target\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:29:08.797233Z","iopub.execute_input":"2025-12-25T10:29:08.798045Z","iopub.status.idle":"2025-12-25T10:29:08.803969Z","shell.execute_reply.started":"2025-12-25T10:29:08.798009Z","shell.execute_reply":"2025-12-25T10:29:08.803109Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"# Create datasets\ntrain_dataset = VQADataset(\n    records=train_records,\n    ans2idx=ans2idx,\n    tokenizer=tokenizer,\n    transform=image_transform,\n    max_len=MAX_QUESTION_LEN\n)\n\nval_dataset = VQADataset(\n    records=val_records,\n    ans2idx=ans2idx,\n    tokenizer=tokenizer,\n    transform=image_transform,\n    max_len=MAX_QUESTION_LEN\n)\n\n# DataLoaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=32,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=32,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\nprint(\"Train batches:\", len(train_loader))\nprint(\"Val batches:\", len(val_loader))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:29:12.227543Z","iopub.execute_input":"2025-12-25T10:29:12.228420Z","iopub.status.idle":"2025-12-25T10:29:12.602642Z","shell.execute_reply.started":"2025-12-25T10:29:12.228389Z","shell.execute_reply":"2025-12-25T10:29:12.601922Z"}},"outputs":[{"name":"stdout","text":"Train batches: 13868\nVal batches: 6692\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"#Quick Check\nlengths = []\n\nfor rec in train_records[:1000]:\n    lengths.append(len(tokenizer.tokenize(rec[\"question\"])))\n\nprint(\"Max length in sample:\", max(lengths))\nprint(\"Average length:\", sum(lengths) / len(lengths))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:29:16.039654Z","iopub.execute_input":"2025-12-25T10:29:16.040392Z","iopub.status.idle":"2025-12-25T10:29:16.149092Z","shell.execute_reply.started":"2025-12-25T10:29:16.040358Z","shell.execute_reply":"2025-12-25T10:29:16.148491Z"}},"outputs":[{"name":"stdout","text":"Max length in sample: 20\nAverage length: 7.572\n","output_type":"stream"}],"execution_count":66},{"cell_type":"markdown","source":"***Model***","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision.models import resnet50\n\nclass ResNetBackbone(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        resnet = resnet50(pretrained=True)\n        \n        # Remove avgpool and fc\n        self.backbone = nn.Sequential(\n            resnet.conv1,\n            resnet.bn1,\n            resnet.relu,\n            resnet.maxpool,\n            resnet.layer1,\n            resnet.layer2,\n            resnet.layer3,\n            resnet.layer4\n        )\n\n    def forward(self, x):\n        # x shape: (batch_size, 3, 224, 224)\n        feat = self.backbone(x)\n        # feat shape: (batch_size, 2048, 7, 7)\n        return feat\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:29:20.492152Z","iopub.execute_input":"2025-12-25T10:29:20.492461Z","iopub.status.idle":"2025-12-25T10:29:20.497756Z","shell.execute_reply.started":"2025-12-25T10:29:20.492435Z","shell.execute_reply":"2025-12-25T10:29:20.496973Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"class SpatialAttention(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        \n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=1,\n            kernel_size=1\n        )\n\n    def forward(self, x):\n        # x shape: (batch_size, 2048, 7, 7)\n        \n        attn_map = self.conv(x)\n        # attn_map shape: (batch_size, 1, 7, 7)\n        \n        attn_map = attn_map.view(attn_map.size(0), -1)\n        # shape: (batch_size, 49)\n        \n        attn_weights = torch.softmax(attn_map, dim=1)\n        # shape: (batch_size, 49)\n        \n        attn_weights = attn_weights.view(x.size(0), 1, 7, 7)\n        # shape: (batch_size, 1, 7, 7)\n        \n        weighted_feat = x * attn_weights\n        # shape: (batch_size, 2048, 7, 7)\n        \n        img_embed = weighted_feat.sum(dim=[2, 3])\n        # img_embed shape: (batch_size, 2048)\n        \n        return img_embed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:29:23.947396Z","iopub.execute_input":"2025-12-25T10:29:23.947723Z","iopub.status.idle":"2025-12-25T10:29:23.953491Z","shell.execute_reply.started":"2025-12-25T10:29:23.947695Z","shell.execute_reply":"2025-12-25T10:29:23.952658Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"from transformers import BertModel\n\nclass TextEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n\n    def forward(self, input_ids, attention_mask):\n        # input_ids shape: (batch_size, max_len)\n        # attention_mask shape: (batch_size, max_len)\n        \n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        cls_embed = outputs.last_hidden_state[:, 0, :]\n        # cls_embed shape: (batch_size, 768)\n        \n        return cls_embed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:29:49.468128Z","iopub.execute_input":"2025-12-25T10:29:49.468533Z","iopub.status.idle":"2025-12-25T10:29:49.473488Z","shell.execute_reply.started":"2025-12-25T10:29:49.468497Z","shell.execute_reply":"2025-12-25T10:29:49.472868Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"class GatedFusion(nn.Module):\n    def __init__(self, img_dim, txt_dim, hidden_dim):\n        super().__init__()\n        \n        self.img_proj = nn.Linear(img_dim, hidden_dim)\n        self.txt_proj = nn.Linear(txt_dim, hidden_dim)\n        \n        self.gate = nn.Linear(hidden_dim * 2, hidden_dim)\n\n    def forward(self, img_feat, txt_feat):\n        # img_feat shape: (batch_size, 2048)\n        # txt_feat shape: (batch_size, 768)\n        \n        img_h = self.img_proj(img_feat)\n        # shape: (batch_size, hidden_dim)\n        \n        txt_h = self.txt_proj(txt_feat)\n        # shape: (batch_size, hidden_dim)\n        \n        concat = torch.cat([img_h, txt_h], dim=1)\n        # shape: (batch_size, hidden_dim * 2)\n        \n        gate = torch.sigmoid(self.gate(concat))\n        # shape: (batch_size, hidden_dim)\n        \n        fused = gate * img_h + (1 - gate) * txt_h\n        # shape: (batch_size, hidden_dim)\n        \n        return fused\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:29:52.836864Z","iopub.execute_input":"2025-12-25T10:29:52.837194Z","iopub.status.idle":"2025-12-25T10:29:52.842557Z","shell.execute_reply.started":"2025-12-25T10:29:52.837165Z","shell.execute_reply":"2025-12-25T10:29:52.841938Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"class VQAModel(nn.Module):\n    def __init__(self, num_answers):\n        super().__init__()\n        \n        self.image_encoder = ResNetBackbone()\n        self.spatial_attn = SpatialAttention(in_channels=2048)\n        self.text_encoder = TextEncoder()\n        \n        self.fusion = GatedFusion(\n            img_dim=2048,\n            txt_dim=768,\n            hidden_dim=1024\n        )\n        \n        self.classifier = nn.Linear(1024, num_answers)\n\n    def forward(self, images, input_ids, attention_mask):\n        # images shape: (batch_size, 3, 224, 224)\n        # input_ids shape: (batch_size, max_len)\n        # attention_mask shape: (batch_size, max_len)\n        \n        img_feat_map = self.image_encoder(images)\n        # shape: (batch_size, 2048, 7, 7)\n        \n        img_feat = self.spatial_attn(img_feat_map)\n        # shape: (batch_size, 2048)\n        \n        txt_feat = self.text_encoder(input_ids, attention_mask)\n        # shape: (batch_size, 768)\n        \n        fused = self.fusion(img_feat, txt_feat)\n        # shape: (batch_size, 1024)\n        \n        logits = self.classifier(fused)\n        # logits shape: (batch_size, 3000)\n        \n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:29:55.447244Z","iopub.execute_input":"2025-12-25T10:29:55.447550Z","iopub.status.idle":"2025-12-25T10:29:55.453220Z","shell.execute_reply.started":"2025-12-25T10:29:55.447523Z","shell.execute_reply":"2025-12-25T10:29:55.452421Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = VQAModel(num_answers=len(ans2idx))\nmodel = model.to(device)\n\nimages, input_ids, attention_mask, targets = next(iter(train_loader))\n\nimages = images.to(device)\ninput_ids = input_ids.to(device)\nattention_mask = attention_mask.to(device)\n\nwith torch.no_grad():\n    outputs = model(images, input_ids, attention_mask)\n\nprint(\"Output logits shape:\", outputs.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:30:00.512139Z","iopub.execute_input":"2025-12-25T10:30:00.512441Z","iopub.status.idle":"2025-12-25T10:30:02.811975Z","shell.execute_reply.started":"2025-12-25T10:30:00.512415Z","shell.execute_reply":"2025-12-25T10:30:02.811243Z"}},"outputs":[{"name":"stdout","text":"Output logits shape: torch.Size([32, 3000])\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"def freeze_module(module):\n    for param in module.parameters():\n        param.requires_grad = False\n\ndef unfreeze_module(module):\n    for param in module.parameters():\n        param.requires_grad = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:30:15.623232Z","iopub.execute_input":"2025-12-25T10:30:15.624125Z","iopub.status.idle":"2025-12-25T10:30:15.628319Z","shell.execute_reply.started":"2025-12-25T10:30:15.624085Z","shell.execute_reply":"2025-12-25T10:30:15.627774Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"# Freeze image encoder and text encoder\nfreeze_module(model.image_encoder)\nfreeze_module(model.text_encoder)\n\nprint(\"Image encoder frozen\")\nprint(\"Text encoder frozen\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:30:18.933969Z","iopub.execute_input":"2025-12-25T10:30:18.934281Z","iopub.status.idle":"2025-12-25T10:30:18.940056Z","shell.execute_reply.started":"2025-12-25T10:30:18.934254Z","shell.execute_reply":"2025-12-25T10:30:18.939332Z"}},"outputs":[{"name":"stdout","text":"Image encoder frozen\nText encoder frozen\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"import torch.optim as optim\n\n# Loss function\n# Expects raw logits and soft targets\ncriterion = nn.BCEWithLogitsLoss()\n\n# Optimizer only sees trainable parameters\noptimizer = optim.Adam(\n    filter(lambda p: p.requires_grad, model.parameters()),\n    lr=1e-4\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:30:27.348067Z","iopub.execute_input":"2025-12-25T10:30:27.348845Z","iopub.status.idle":"2025-12-25T10:30:27.354443Z","shell.execute_reply.started":"2025-12-25T10:30:27.348815Z","shell.execute_reply":"2025-12-25T10:30:27.353807Z"}},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":"***Traning Loop***","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(model, dataloader, optimizer, criterion, device, epoch, phase):\n    model.train()\n    \n    total_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for batch_idx, (images, input_ids, attention_mask, targets) in enumerate(dataloader):\n        images = images.to(device)\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        targets = targets.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(images, input_ids, attention_mask)\n        # outputs shape: (batch_size, 3000)\n        \n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        # Accuracy calculation\n        preds = outputs.argmax(dim=1)\n        gt = targets.argmax(dim=1)\n        \n        correct += (preds == gt).sum().item()\n        total += targets.size(0)\n        \n        # Print batch progress every 200 batches\n        if batch_idx % 200 == 0:\n            print(\n                f\"{phase} Epoch {epoch} | \"\n                f\"Batch {batch_idx}/{len(dataloader)} | \"\n                f\"Loss {loss.item():.4f}\"\n            )\n    \n    avg_loss = total_loss / len(dataloader)\n    acc = correct / total\n    \n    return avg_loss, acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:31:23.663543Z","iopub.execute_input":"2025-12-25T10:31:23.664389Z","iopub.status.idle":"2025-12-25T10:31:23.670950Z","shell.execute_reply.started":"2025-12-25T10:31:23.664359Z","shell.execute_reply":"2025-12-25T10:31:23.670222Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"def validate_one_epoch(model, dataloader, criterion, device):\n    model.eval()\n    \n    total_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, input_ids, attention_mask, targets in dataloader:\n            images = images.to(device)\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            targets = targets.to(device)\n            \n            outputs = model(images, input_ids, attention_mask)\n            loss = criterion(outputs, targets)\n            \n            total_loss += loss.item()\n            \n            preds = outputs.argmax(dim=1)\n            gt = targets.argmax(dim=1)\n            \n            correct += (preds == gt).sum().item()\n            total += targets.size(0)\n    \n    avg_loss = total_loss / len(dataloader)\n    acc = correct / total\n    \n    return avg_loss, acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:31:28.377037Z","iopub.execute_input":"2025-12-25T10:31:28.377702Z","iopub.status.idle":"2025-12-25T10:31:28.383321Z","shell.execute_reply.started":"2025-12-25T10:31:28.377671Z","shell.execute_reply":"2025-12-25T10:31:28.382668Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"def save_checkpoint(model, optimizer, epoch, path):\n    checkpoint = {\n        \"epoch\": epoch,\n        \"model_state\": model.state_dict(),\n        \"optimizer_state\": optimizer.state_dict()\n    }\n    torch.save(checkpoint, path)\n\ndef load_checkpoint(model, optimizer, path, device):\n    checkpoint = torch.load(path, map_location=device)\n    model.load_state_dict(checkpoint[\"model_state\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n    start_epoch = checkpoint[\"epoch\"] + 1\n    return start_epoch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:31:32.662117Z","iopub.execute_input":"2025-12-25T10:31:32.662416Z","iopub.status.idle":"2025-12-25T10:31:32.667784Z","shell.execute_reply.started":"2025-12-25T10:31:32.662389Z","shell.execute_reply":"2025-12-25T10:31:32.666944Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"train_loss_history = []\ntrain_acc_history = []\nval_loss_history = []\nval_acc_history = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:31:37.352119Z","iopub.execute_input":"2025-12-25T10:31:37.352829Z","iopub.status.idle":"2025-12-25T10:31:37.356435Z","shell.execute_reply.started":"2025-12-25T10:31:37.352797Z","shell.execute_reply":"2025-12-25T10:31:37.355652Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"num_epochs_frozen = 3\nnum_epochs_unfrozen = 2\n\ncheckpoint_path = \"/kaggle/working/vqa_checkpoint.pth\"\n\nstart_epoch = 0\n\nfor epoch in range(start_epoch, num_epochs_frozen):\n    train_loss, train_acc = train_one_epoch(\n        model=model,\n        dataloader=train_loader,\n        optimizer=optimizer,\n        criterion=criterion,\n        device=device,\n        epoch=epoch,\n        phase=\"Frozen\"\n    )\n    \n    val_loss, val_acc = validate_one_epoch(\n        model=model,\n        dataloader=val_loader,\n        criterion=criterion,\n        device=device\n    )\n    \n    train_loss_history.append(train_loss)\n    train_acc_history.append(train_acc)\n    val_loss_history.append(val_loss)\n    val_acc_history.append(val_acc)\n    \n    print(\n        f\"Frozen Epoch {epoch} | \"\n        f\"Train Loss {train_loss:.4f} | Train Acc {train_acc:.4f} | \"\n        f\"Val Loss {val_loss:.4f} | Val Acc {val_acc:.4f}\"\n        \n    )\n    \n    save_checkpoint(\n        model=model,\n        optimizer=optimizer,\n        epoch=epoch,\n        path=checkpoint_path\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:40:32.682526Z","iopub.execute_input":"2025-12-25T10:40:32.683365Z"}},"outputs":[{"name":"stdout","text":"Frozen Epoch 0 | Batch 0/13868 | Loss 0.0020\nFrozen Epoch 0 | Batch 200/13868 | Loss 0.0024\nFrozen Epoch 0 | Batch 400/13868 | Loss 0.0016\nFrozen Epoch 0 | Batch 600/13868 | Loss 0.0016\nFrozen Epoch 0 | Batch 800/13868 | Loss 0.0019\nFrozen Epoch 0 | Batch 1000/13868 | Loss 0.0015\nFrozen Epoch 0 | Batch 1200/13868 | Loss 0.0017\nFrozen Epoch 0 | Batch 1400/13868 | Loss 0.0015\nFrozen Epoch 0 | Batch 1600/13868 | Loss 0.0017\nFrozen Epoch 0 | Batch 1800/13868 | Loss 0.0014\nFrozen Epoch 0 | Batch 2000/13868 | Loss 0.0013\nFrozen Epoch 0 | Batch 2200/13868 | Loss 0.0014\nFrozen Epoch 0 | Batch 2400/13868 | Loss 0.0014\nFrozen Epoch 0 | Batch 2600/13868 | Loss 0.0015\nFrozen Epoch 0 | Batch 2800/13868 | Loss 0.0011\nFrozen Epoch 0 | Batch 3000/13868 | Loss 0.0011\nFrozen Epoch 0 | Batch 3200/13868 | Loss 0.0014\nFrozen Epoch 0 | Batch 3400/13868 | Loss 0.0012\nFrozen Epoch 0 | Batch 3600/13868 | Loss 0.0013\nFrozen Epoch 0 | Batch 3800/13868 | Loss 0.0012\nFrozen Epoch 0 | Batch 4000/13868 | Loss 0.0012\nFrozen Epoch 0 | Batch 4200/13868 | Loss 0.0015\nFrozen Epoch 0 | Batch 4400/13868 | Loss 0.0012\nFrozen Epoch 0 | Batch 4600/13868 | Loss 0.0012\nFrozen Epoch 0 | Batch 4800/13868 | Loss 0.0014\nFrozen Epoch 0 | Batch 5000/13868 | Loss 0.0013\nFrozen Epoch 0 | Batch 5200/13868 | Loss 0.0014\nFrozen Epoch 0 | Batch 5400/13868 | Loss 0.0013\nFrozen Epoch 0 | Batch 5600/13868 | Loss 0.0013\nFrozen Epoch 0 | Batch 5800/13868 | Loss 0.0012\nFrozen Epoch 0 | Batch 6000/13868 | Loss 0.0012\nFrozen Epoch 0 | Batch 6200/13868 | Loss 0.0010\nFrozen Epoch 0 | Batch 6400/13868 | Loss 0.0012\nFrozen Epoch 0 | Batch 6600/13868 | Loss 0.0012\nFrozen Epoch 0 | Batch 6800/13868 | Loss 0.0010\nFrozen Epoch 0 | Batch 7000/13868 | Loss 0.0012\nFrozen Epoch 0 | Batch 7200/13868 | Loss 0.0011\nFrozen Epoch 0 | Batch 7400/13868 | Loss 0.0011\nFrozen Epoch 0 | Batch 7600/13868 | Loss 0.0014\nFrozen Epoch 0 | Batch 7800/13868 | Loss 0.0011\nFrozen Epoch 0 | Batch 8000/13868 | Loss 0.0010\nFrozen Epoch 0 | Batch 8200/13868 | Loss 0.0012\nFrozen Epoch 0 | Batch 8400/13868 | Loss 0.0011\nFrozen Epoch 0 | Batch 8600/13868 | Loss 0.0012\nFrozen Epoch 0 | Batch 8800/13868 | Loss 0.0009\nFrozen Epoch 0 | Batch 9000/13868 | Loss 0.0010\nFrozen Epoch 0 | Batch 9200/13868 | Loss 0.0011\nFrozen Epoch 0 | Batch 9400/13868 | Loss 0.0011\nFrozen Epoch 0 | Batch 9600/13868 | Loss 0.0010\nFrozen Epoch 0 | Batch 9800/13868 | Loss 0.0013\nFrozen Epoch 0 | Batch 10000/13868 | Loss 0.0012\nFrozen Epoch 0 | Batch 10200/13868 | Loss 0.0011\nFrozen Epoch 0 | Batch 10400/13868 | Loss 0.0011\nFrozen Epoch 0 | Batch 10600/13868 | Loss 0.0009\nFrozen Epoch 0 | Batch 10800/13868 | Loss 0.0012\nFrozen Epoch 0 | Batch 11000/13868 | Loss 0.0012\nFrozen Epoch 0 | Batch 11200/13868 | Loss 0.0012\nFrozen Epoch 0 | Batch 11400/13868 | Loss 0.0010\nFrozen Epoch 0 | Batch 11600/13868 | Loss 0.0009\nFrozen Epoch 0 | Batch 11800/13868 | Loss 0.0008\nFrozen Epoch 0 | Batch 12000/13868 | Loss 0.0009\nFrozen Epoch 0 | Batch 12200/13868 | Loss 0.0011\nFrozen Epoch 0 | Batch 12400/13868 | Loss 0.0012\nFrozen Epoch 0 | Batch 12600/13868 | Loss 0.0011\nFrozen Epoch 0 | Batch 12800/13868 | Loss 0.0010\nFrozen Epoch 0 | Batch 13000/13868 | Loss 0.0011\nFrozen Epoch 0 | Batch 13200/13868 | Loss 0.0012\nFrozen Epoch 0 | Batch 13400/13868 | Loss 0.0009\nFrozen Epoch 0 | Batch 13600/13868 | Loss 0.0012\nFrozen Epoch 0 | Batch 13800/13868 | Loss 0.0009\nFrozen Epoch 0 | Train Loss 0.0012 | Train Acc 0.2956 | Val Loss 0.0010 | Val Acc 0.3486\nFrozen Epoch 1 | Batch 0/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 200/13868 | Loss 0.0013\nFrozen Epoch 1 | Batch 400/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 600/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 800/13868 | Loss 0.0011\nFrozen Epoch 1 | Batch 1000/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 1200/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 1400/13868 | Loss 0.0012\nFrozen Epoch 1 | Batch 1600/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 1800/13868 | Loss 0.0011\nFrozen Epoch 1 | Batch 2000/13868 | Loss 0.0011\nFrozen Epoch 1 | Batch 2200/13868 | Loss 0.0011\nFrozen Epoch 1 | Batch 2400/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 2600/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 2800/13868 | Loss 0.0011\nFrozen Epoch 1 | Batch 3000/13868 | Loss 0.0009\nFrozen Epoch 1 | Batch 3200/13868 | Loss 0.0009\nFrozen Epoch 1 | Batch 3400/13868 | Loss 0.0009\nFrozen Epoch 1 | Batch 3600/13868 | Loss 0.0014\nFrozen Epoch 1 | Batch 3800/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 4000/13868 | Loss 0.0008\nFrozen Epoch 1 | Batch 4200/13868 | Loss 0.0009\nFrozen Epoch 1 | Batch 4400/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 4600/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 4800/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 5000/13868 | Loss 0.0007\nFrozen Epoch 1 | Batch 5200/13868 | Loss 0.0011\nFrozen Epoch 1 | Batch 5400/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 5600/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 5800/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 6000/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 6200/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 6400/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 6600/13868 | Loss 0.0008\nFrozen Epoch 1 | Batch 6800/13868 | Loss 0.0009\nFrozen Epoch 1 | Batch 7000/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 7200/13868 | Loss 0.0009\nFrozen Epoch 1 | Batch 7400/13868 | Loss 0.0009\nFrozen Epoch 1 | Batch 7600/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 7800/13868 | Loss 0.0009\nFrozen Epoch 1 | Batch 8000/13868 | Loss 0.0009\nFrozen Epoch 1 | Batch 8200/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 8400/13868 | Loss 0.0009\nFrozen Epoch 1 | Batch 8600/13868 | Loss 0.0012\nFrozen Epoch 1 | Batch 8800/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 9000/13868 | Loss 0.0009\nFrozen Epoch 1 | Batch 9200/13868 | Loss 0.0011\nFrozen Epoch 1 | Batch 9400/13868 | Loss 0.0009\nFrozen Epoch 1 | Batch 9600/13868 | Loss 0.0011\nFrozen Epoch 1 | Batch 9800/13868 | Loss 0.0009\nFrozen Epoch 1 | Batch 10000/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 10200/13868 | Loss 0.0009\nFrozen Epoch 1 | Batch 10400/13868 | Loss 0.0009\nFrozen Epoch 1 | Batch 10600/13868 | Loss 0.0011\nFrozen Epoch 1 | Batch 10800/13868 | Loss 0.0012\nFrozen Epoch 1 | Batch 11000/13868 | Loss 0.0008\nFrozen Epoch 1 | Batch 11200/13868 | Loss 0.0009\nFrozen Epoch 1 | Batch 11400/13868 | Loss 0.0009\nFrozen Epoch 1 | Batch 11600/13868 | Loss 0.0011\nFrozen Epoch 1 | Batch 11800/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 12000/13868 | Loss 0.0010\nFrozen Epoch 1 | Batch 12200/13868 | Loss 0.0012\nFrozen Epoch 1 | Batch 12400/13868 | Loss 0.0009\nFrozen Epoch 1 | Batch 12600/13868 | Loss 0.0009\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"unfreeze_module(model.image_encoder)\nunfreeze_module(model.text_encoder)\n\noptimizer = torch.optim.Adam(\n    model.parameters(),\n    lr=1e-5\n)\n\nprint(\"Encoders unfrozen and optimizer reset\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(num_epochs_frozen, num_epochs_frozen + num_epochs_unfrozen):\n    train_loss, train_acc = train_one_epoch(\n        model=model,\n        dataloader=train_loader,\n        optimizer=optimizer,\n        criterion=criterion,\n        device=device,\n        epoch=epoch,\n        phase=\"Unfrozen\"\n    )\n    \n    val_loss, val_acc = validate_one_epoch(\n        model=model,\n        dataloader=val_loader,\n        criterion=criterion,\n        device=device\n    )\n    \n    # Save metrics\n    train_loss_history.append(train_loss)\n    train_acc_history.append(train_acc)\n    val_loss_history.append(val_loss)\n    val_acc_history.append(val_acc)\n    \n    print(\n        f\"Unfrozen Epoch {epoch} | \"\n        f\"Train Loss {train_loss:.4f} | Train Acc {train_acc:.4f} | \"\n        f\"Val Loss {val_loss:.4f} | Val Acc {val_acc:.4f}\"\n    )\n    \n    save_checkpoint(\n        model=model,\n        optimizer=optimizer,\n        epoch=epoch,\n        path=checkpoint_path\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***For Saving Model***","metadata":{}},{"cell_type":"code","source":"final_model_path = \"/kaggle/working/vqa_final_model.pth\"\n\ntorch.save({\n    \"model_state\": model.state_dict(),\n    \"ans2idx\": ans2idx,\n    \"idx2ans\": idx2ans,\n    \"max_question_len\": MAX_QUESTION_LEN\n}, final_model_path)\n\nprint(\"Final trained model saved at:\", final_model_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}